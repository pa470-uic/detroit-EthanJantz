---
title: "Property Assessment in Detroit"
author: "Ethan Jantz"
date: "3/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
library(tidymodels)
library(tidyverse)
library(lubridate)
library(leaflet)

options(scipen = 999)

`%nin%` <- Negate(`%in%`)

# Database provided by teacher
con <- DBI::dbConnect(RSQLite::SQLite(), here::here("database", "detroit.sqlite"))

DBI::dbListTables(con) # assessments, blight, foreclosures, parcels, parcels_historic, sales

sales <- tbl(con, 'sales') %>% 
  collect() %>%
  mutate(
    across(c(grantor, grantee, sale_terms, ecf, property_c), as.factor),
    # parcel_num = str_remove(parcel_num, "\\.$"),
    sale_date = lubridate::as_date(sale_date),
    maybe_not_armslength = ifelse(sale_price %in% c(0, 1), T, F),
    sale_price = as.numeric(sale_price),
    sale_price_r = round(sale_price, digits = -2),
    sale_lessthan_10k = ifelse(sale_price < 10000, T, F),
    sale_year = year(sale_date)
  )

assessments <- tbl(con, 'assessments') %>% 
  collect() %>%
  mutate(
    # PARCELNO = str_remove(PARCELNO, "\\.$"),
    year = as_date(paste(year, 1, 1, sep = "-"))
  )

blight <- tbl(con, 'blight') %>% 
  collect() %>%
  mutate(
    across(c(agency_name, violator_name, city, state, zip_code, payment_status, violation_code),
           as.factor),
    across(c(violation_date, payment_date),
           ~ str_extract(.x, "[0-9]{4}/[0-9]{2}/[0-9]{2}") %>% 
             str_replace("/", "-") %>%
             as_date()),
    ticket_issued_time = str_remove(ticket_issued_time, "\\+[0-9]{2}$") %>%
      str_remove("[0-9]{4}/[0-9]{2}/[0-9]{2} ") %>% 
      hms()
    # parcelno = str_remove(parcelno, "\\.$")
  )

foreclosures <- tbl(con, 'foreclosures') %>% 
  collect() #%>%
  # mutate(prop_parcelnum = str_remove(prop_parcelnum, "\\.$"))

foreclosures_sum <- foreclosures %>%
  mutate(
    across(where(is.numeric), ~ifelse(is.na(.x), 0, .x)),
    foreclosures = rowSums(across(where(is.numeric)))
    ) %>%
  select(where(is.character), foreclosures)

parcels <- tbl(con, 'parcels') %>% 
  collect() %>%
  mutate(
    # across(c(parcel_number, related), ~str_remove(.x, "\\.$")),
    sale_date = as_datetime(sale_date),
    sale_year = year(sale_date)
    )

parcels_historic <- tbl(con, 'parcels_historic') %>% collect()

attributes <- tbl(con, 'attributes') %>% 
  collect() %>%
  mutate(
    extcat = case_when(
      extcat == 1 ~ "Siding",
      extcat == 2 ~ "Brick/Other",
      extcat == 3 ~ "Brick",
      extcat == 4 ~ "Other",
      TRUE ~ NA_character_
    ),
    bathcat = case_when(
      bathcat == 1 ~ "1",
      bathcat == 2 ~ "1.5",
      bathcat == 3 ~ "2-3",
      bathcat == 4 ~ "4+",
      TRUE ~ NA_character_
    ),
    heightcat = case_when(
      heightcat == 1 ~ "1-1.5",
      heightcat == 2 ~ "1.5-2.5",
      heightcat == 3 ~ "3+",
      TRUE ~ NA_character_
    )
  )

# Recreate sales ratio study from part 1
sales_study <- sales %>%
  select(parcel_num, sale_year, sale_price) %>%
  filter(sale_year %in% c(2010:2019)) %>%
  distinct(parcel_num, .keep_all = T) %>%
  filter(sale_price > 2500)

study_data <- sales_study %>%
  left_join(assessments %>% 
              select(PARCELNO, year, ASSESSEDVALUE) %>%
              mutate(year = year(year)) %>%
              filter(ASSESSEDVALUE > 1250),
            by = c("parcel_num" = "PARCELNO", "sale_year" = "year"))

ratios <- cmfproperty::reformat_data(
  data = study_data,
  sale_col = "sale_price",
  assessment_col = "ASSESSEDVALUE",
  sale_year_col = "sale_year"
)

stats <- cmfproperty::calc_iaao_stats(ratios)

output <- cmfproperty::diagnostic_plots(stats, ratios, min_reporting_yr = 2010, max_reporting_yr = 2019)
```

# Introduction

Detroit's residential property stock has been in decline since 2014, which is around the time that home sales fell to their lowest point in the 2010's. This decline in single family housing stock began when Detroit's housing market saw the largest wave of foreclosures it had seen in decades. Meanwhile, home sales have continued on an upward trend since at least 2011.

```{r plot 1}
properties <- assessments %>%
  count(year) %>%
  filter(year < as_date("2022-01-01"))

homes <- assessments %>% 
  filter(propclass == 401) %>%
  count(year) %>%
  filter(year < as_date("2022-01-01"))

ggplot(data = NULL, aes(x = year, y = n)) +
  geom_line(data = properties, color = "black", size = 1, alpha = .5) +
  geom_point(data = properties, color = "black", alpha = .5) +
  geom_line(data = homes, color = "blue", size = 1, alpha = .5) +
  geom_point(data = homes, color = "blue", alpha = .5) +
  scale_x_date(date_labels = "%Y", date_breaks = "2 year") +
  scale_y_continuous(labels = scales::comma,
                     limits = c(0, 355000)) +
  labs(title = "While the number of properties in Detroit has increased\nthe number of single-family homes (blue) has dropped by a third",
       x = "Year", y = "Properties") +
  theme_minimal()
```

```{r plot 2}
sales %>%
  count(sale_year) %>%
  filter(sale_year < 2020) %>%
  ggplot(aes(x = sale_year, y = n)) +
  geom_col() +
  scale_x_continuous(breaks = c(2011:2019)) +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Year", y = "Homes Sold",
       title = "Home sales have been increasing in volume since 2011") +
  theme_minimal()
```

```{r plot 3}
foreclosures %>%
  pivot_longer(
    cols = c(`2002`:`2019`),
    names_to = "year"
  ) %>%
  group_by(year) %>%
  summarize(foreclosures = sum(value, na.rm = T)) %>%
  ggplot(aes(x = year, y = foreclosures)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Year", y = "Homes Sold",
       title = "Foreclosures peaked in 2015") +
  theme_minimal()
```

Detroit is the city with the highest poverty rate in the country. It has been [well documented](https://www.habitatbuilds.com/wp-content/uploads/2016/04/Benefits-of-Homeownership-Research-Summary.pdf) that homeownership is one of the many critical footholds for families to escape poverty, and property taxes can often make or break a family's ability to own a home. Knowing this, it is important for assessors offices to provide accurate assessments of properties so tax burdens are shared evenly across property owners and low-income households are not disproportionately burdened.

The State of Michigan requires assessors to assess properties at 50% of their true, or market, value. Detroit's assessments have failed to meet this requirement, though they ave been improving since 2012. Most assessments since 2016 have been below 100% of the market value. 

```{r plot 4}
output[[3]] +
  labs(title = "Assessment ratios have improved since 2012")
```

## Predicting Overassessments

This report aims to build a better assessment model than the one that exists currently using machine learning techniques. Before attempting to predict the sale price of a given home, we will first build a model for predicting which homes have been overassessed, or assessed above 50% of its market value per Michigan law.

This model was created in the previous iteration of the assignment, and this iteration will include foreclosures as a predictor. This random forest model predicts over-assessments using the sale price, ZIP code, whether or not the property had been improved, the type of property (single family, two-flat, etc), the zoning code the property falls under, the percentage of population that identified as Black/African-American in 2019 according to the American Community Survey, and the number of times the property had been foreclosed. The model uses properties assessed in 2016.

The modeling data removes properties that sold for less than 2500 or were assessed at less than 1250 to remove as many sales that were not arms-length sales as possible. 

```{r rf 0}
rm(sales_study)
rm(study_data)

model_data <- assessments %>%
  left_join(
    sales,
    by = c("PARCELNO" = "parcel_num")
) %>%
  # Introducing new predictors
  left_join(
    foreclosures_sum,
    by = c("PARCELNO" = "prop_parcelnum")
  )

model_data <- model_data %>%
  filter(year == "2016-01-01", 
         property_c == "401",
         sale_price > 2500,
         ASSESSEDVALUE > 1250) %>%
  mutate(over = ifelse(ASSESSEDVALUE > (sale_price / 2), 
                       "Over", "Not") %>% 
           as.factor())

model_data <- model_data %>%
  left_join(
    parcels %>%
      select(parcel_number, ward, zip_code, use_code_desc, total_square_footage, is_improved, style, year_built, zoning, lon = X, lat = Y),
    by = c("PARCELNO" = "parcel_number")
  ) %>%
  mutate(is_improved = ifelse(is_improved == 1, T, F)) %>%
  filter(complete.cases(.))

rf_model_data_sf <- model_data %>%
  sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  sf::st_join(
    tidycensus::get_acs("tract",
                        variables = c(pop = "B02001_001",
                                      bpop = "B02001_003"),
                        state = "MI",
                        county = "Wayne",
                        output = "wide",
                        geometry = T) %>%
      select(GEOID, pop = popE, bpop = bpopE) %>%
      mutate(bpct = bpop / pop,
             bpct = ifelse(is.nan(bpct), 0, bpct)) %>%
      sf::st_transform(crs = 4326)
  )

rm(model_data)
```

```{r rf 1}
split <- initial_split(rf_model_data_sf)
train <- training(split)
test  <- testing(split)

rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_recipe <- recipe(over ~ sale_price + zip_code + is_improved + style + zoning + foreclosures + bpct, 
                    data = train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors())

rf_workflow <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_recipe)

rf_fit <- fit(rf_workflow, train)

rf_preds <- augment(rf_fit, train)

rm(split, rf_workflow, rf_recipe)
```

<!-- ```{r rf 2} -->
<!-- performanceEstimation::classificationMetrics(rf_preds$over, rf_preds$.pred_class, metrics = c("fpr", "fnr", "tpr", "tnr", "ppv")) -->
<!-- ``` -->

```{r rf 3}
rf_test <- augment(rf_fit, test)

rf_test %>%
  select(PARCELNO, over, .pred_class) %>%
  count(over, .pred_class) %>%
  mutate(pct = n / sum(n))

performanceEstimation::classificationMetrics(rf_test$over, rf_test$.pred_class, metrics = c("fpr", "fnr", "tpr", "tnr", "ppv"))
```
The following ROC plot shows that the model performs well in testing, predicting over 85% of cases correctly in the test data. The inclusion of demographic and foreclosure data did not significantly improve the model, though the model performs well both with and without the new data. The ROC plot below shows this performance, where sensitivity is the proportion of true positive cases and specificity is the proportion of true negative cases.

```{r rf 4}
roc <- yardstick::roc_curve(data = rf_test, truth = over, .pred_Not)

autoplot(roc)
```

## Predicting Assessed Values

This model uses the same modeling data process as the over-assessment predictor model with a minor change. The data used in assessment predictions is going to include data from 2016 until 2019. It trains on data from 2016 to 2018 and tests on data from 2019.

```{r lm 0}
model_data <- assessments %>%
  left_join(
    sales,
    by = c("PARCELNO" = "parcel_num")
) %>%
  left_join(
    foreclosures_sum,
    by = c("PARCELNO" = "prop_parcelnum")
  )

model_data <- model_data %>%
  filter(year(year) <= 2019, year(year) >= 2016, 
         year(sale_date) %in% c(2016:2019),
         property_c == "401",
         sale_price > 2500,
         ASSESSEDVALUE > 1250) %>%
  mutate(over = ifelse(ASSESSEDVALUE > (sale_price / 2), 
                       "Over", "Not") %>% 
           as.factor())

model_data <- model_data %>%
  left_join(
    parcels %>%
      select(parcel_number, ward, zip_code, use_code_desc, total_square_footage, is_improved, style, year_built, zoning, lon = X, lat = Y),
    by = c("PARCELNO" = "parcel_number")
  ) %>%
  mutate(is_improved = ifelse(is_improved == 1, T, F)) %>%
  filter(complete.cases(.))

lm_model_data_sf <- model_data %>%
  sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  sf::st_join(
    tidycensus::get_acs("tract",
                        variables = c(pop = "B02001_001",
                                      bpop = "B02001_003"),
                        state = "MI",
                        county = "Wayne",
                        output = "wide",
                        geometry = T) %>%
      select(GEOID, pop = popE, bpop = bpopE) %>%
      mutate(bpct = bpop / pop,
             bpct = ifelse(is.nan(bpct), 0, bpct)) %>%
      sf::st_transform(crs = 4326)
  ) %>%
  arrange(year)

rm(model_data)
```

```{r lm 1}
split <- initial_time_split(lm_model_data_sf)
train <- training(split)
test  <- testing(split)

lm_mod <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

lm_recipe <- recipe(ASSESSEDVALUE ~ sale_price + total_square_footage + 
                      year_built + is_improved + style + foreclosures + bpct,
                    data = train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors())

lm_workflow <- workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(lm_recipe)

lm_fit <- fit(lm_workflow, train)

lm_preds <- augment(lm_fit, train)

rm(split, lm_workflow, lm_recipe)
```

```{r lm 2}
bind_rows(
  mape(lm_preds, truth = sale_price, estimate = .pred),
  rmse(lm_preds, truth = sale_price, estimate = .pred)
)
```
Before the introduction of new features, the model performed with an average of 72% off across all predictions, and an RMSE of $95,000. Now the model has an MAPE of 79% and an RMSE of $64,000.

```{r lm 3}
lm_preds <- augment(lm_fit, test %>% filter(year == "2019-01-01"))

bind_rows(
  mape(lm_preds, truth = sale_price, estimate = .pred),
  rmse(lm_preds, truth = sale_price, estimate = .pred)
)
```

## Generating Out of Sample Predictions

This section will run the same models as above, but will focus on generating out of sample predictions. This means that, for the year being predicted, the models will predict for homes that did not sell. This will require minor changes to the model, such as replacing sale price (a value that won't exist for homes that didn't sell) with assessed values. 

### Over-Assessments

The over-assessment model trains on data where a sale was made in 2016 and tests on all homes that didn't sell that year. 

```{r rf 1}
train <- rf_model_data_sf %>%
  filter(sale_year == 2016)
test  <- rf_model_data_sf %>%
  filter(sale_year != 2016)

rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_recipe <- recipe(over ~ sale_price + zip_code + is_improved + style + zoning + foreclosures + bpct, 
                    data = train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors())

rf_workflow <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_recipe)

rf_fit <- fit(rf_workflow, train)

rf_preds <- augment(rf_fit, train)
```

This model performs as well at out of sample predictions as the previous version of the model, with a 91% positive predictive value rate. 

```{r rf 3}
rf_test <- augment(rf_fit, test)

performanceEstimation::classificationMetrics(rf_test$over, rf_test$.pred_class, metrics = c("fpr", "fnr", "tpr", "tnr", "ppv"))
```
The following map shows the average true positive rate by census tract. 

```{r map}
tpr_sf <- rf_test %>%
  select(GEOID, over, .pred_class) %>%
  count(GEOID, over, .pred_class) %>%
  group_by(GEOID) %>%
  summarize(tpr = n / sum(n)) %>%
  left_join(
    tigris::tracts(state = "MI",
                   county = "Wayne",
                   cb = T) %>%
      select(GEOID),
    by = "GEOID"
  ) %>%
  ungroup() %>%
  sf::st_as_sf()

pal <- colorQuantile("YlOrRd", domain = tpr_sf$tpr)

leaflet(tpr_sf) %>%
  addPolygons(fillColor = ~pal(tpr),
              color = "white",
              weight = 2) %>%
  addProviderTiles("CartoDB.Positron")
```

And the following is a correlation plot of the relationship between the model's true positivity rate and the percentage of Black/African-American people living in the associated census tract. There does not seem to be a correlation between the two.

```{r corrplot}
df <- tpr_sf %>%
  left_join(
    tidycensus::get_acs("tract",
                        variables = c(pop = "B02001_001",
                                      bpop = "B02001_003"),
                        state = "MI",
                        county = "Wayne",
                        output = "wide",
                        geometry = F) %>%
      select(GEOID, pop = popE, bpop = bpopE) %>%
      mutate(bpct = bpop / pop,
             bpct = ifelse(is.nan(bpct), 0, bpct)) 
  ) %>%
  sf::st_drop_geometry() %>%
  select(tpr, bpct)

df %>%
  ggplot(aes(x = bpct, y = tpr)) +
  geom_jitter() +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Correlation Plot",
       subtitle = "Model True Positivity Rate and Percentage of Black/African-American\nPopulation in Corresponding Census Tract",
       x = "Percent Black/African-American Population",
       y = "True Positivity Rate")
```

### Predicting Assessments