---
title: "Part 2"
author: "Ethan Jantz"
date: "2/27/2022"
output:
  pdf_document: default
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
library(tidymodels)
library(tidyverse)
library(lubridate)

options(scipen = 999)

`%nin%` <- Negate(`%in%`)

# Database provided by teacher
con <- DBI::dbConnect(RSQLite::SQLite(), here::here("database", "detroit.sqlite"))

# DBI::dbListTables(con) # assessments, blight, foreclosures, parcels, parcels_historic, sales

sales <- tbl(con, 'sales') %>% 
  collect() %>%
  mutate(
    across(c(grantor, grantee, sale_terms, ecf, property_c), as.factor),
    # parcel_num = str_remove(parcel_num, "\\.$"),
    sale_date = lubridate::as_date(sale_date),
    maybe_not_armslength = ifelse(sale_price %in% c(0, 1), T, F),
    sale_price = as.numeric(sale_price),
    sale_price_r = round(sale_price, digits = -2),
    sale_lessthan_10k = ifelse(sale_price < 10000, T, F),
    sale_year = year(sale_date)
  )

assessments <- tbl(con, 'assessments') %>% 
  collect() %>%
  mutate(
    # PARCELNO = str_remove(PARCELNO, "\\.$"),
    year = as_date(paste(year, 1, 1, sep = "-"))
  )

blight <- tbl(con, 'blight') %>% 
  collect() %>%
  mutate(
    across(c(agency_name, violator_name, city, state, zip_code, payment_status, violation_code),
           as.factor),
    across(c(violation_date, payment_date),
           ~ str_extract(.x, "[0-9]{4}/[0-9]{2}/[0-9]{2}") %>% 
             str_replace("/", "-") %>%
             as_date()),
    ticket_issued_time = str_remove(ticket_issued_time, "\\+[0-9]{2}$") %>%
      str_remove("[0-9]{4}/[0-9]{2}/[0-9]{2} ") %>% 
      hms()
    # parcelno = str_remove(parcelno, "\\.$")
  )

foreclosures <- tbl(con, 'foreclosures') %>% 
  collect() #%>%
  # mutate(prop_parcelnum = str_remove(prop_parcelnum, "\\.$"))

foreclosures_sum <- foreclosures %>%
  mutate(
    across(where(is.numeric), ~ifelse(is.na(.x), 0, .x)),
    foreclosures = rowSums(across(where(is.numeric)))
    ) %>%
  select(where(is.character), foreclosures)

parcels <- tbl(con, 'parcels') %>% 
  collect() %>%
  mutate(
    # across(c(parcel_number, related), ~str_remove(.x, "\\.$")),
    sale_date = as_datetime(sale_date),
    sale_year = year(sale_date)
    )

parcels_historic <- tbl(con, 'parcels_historic') %>% collect()

# Recreate sales ratio study from part 1
sales_study <- sales %>%
  select(parcel_num, sale_year, sale_price) %>%
  filter(sale_year %in% c(2010:2019)) %>%
  distinct(parcel_num, .keep_all = T) %>%
  filter(sale_price > 2500)

study_data <- sales_study %>%
  left_join(assessments %>% 
              select(PARCELNO, year, ASSESSEDVALUE) %>%
              mutate(year = year(year)) %>%
              filter(ASSESSEDVALUE > 1250),
            by = c("parcel_num" = "PARCELNO", "sale_year" = "year"))

ratios <- cmfproperty::reformat_data(
  data = study_data,
  sale_col = "sale_price",
  assessment_col = "ASSESSEDVALUE",
  sale_year_col = "sale_year"
)

stats <- cmfproperty::calc_iaao_stats(ratios)

output <- cmfproperty::diagnostic_plots(stats, ratios, min_reporting_yr = 2010, max_reporting_yr = 2019)
```

# Introduction

Detroit's residential property stock has been in decline since 2014, which is around the time that home sales fell to their lowest point in the 2010's. This coincides with the highest rate of foreclosures in the city in the decade. Detroit aims to assess homes at 50% of their value, and while there has been progress toward this metric assessments continue to 

```{r plot 1}
properties <- assessments %>%
  count(year) %>%
  filter(year < as_date("2022-01-01"))

homes <- assessments %>% 
  filter(propclass == 401) %>%
  count(year) %>%
  filter(year < as_date("2022-01-01"))

ggplot(data = NULL, aes(x = year, y = n)) +
  geom_line(data = properties, color = "black", size = 2, alpha = .5) +
  geom_line(data = homes, color = "blue", size = 2, alpha = .5) +
  scale_x_date(date_labels = "%Y", date_breaks = "2 year") +
  scale_y_continuous(labels = scales::comma,
                     limits = c(0, 355000)) +
  labs(title = "While the number of properties in Detroit has increased\nthe number of homes (blue) has dropped by a third",
       x = "Year", y = "Properties") +
  theme_minimal()
```

```{r plot 2}
parcels %>%
  mutate(sale_year = year(sale_date)) %>%
  select(sale_year, sale_price, assessed_value, property_class) %>%
  filter(sale_price > 4000, assessed_value > 2000, sale_year >= 1990, sale_year <= 2020, property_class == "401") %>%
  ggplot(aes(x = sale_year)) +
  geom_bar() +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Year", y = "Homes Sold",
       title = "Home sales have increased in volume since 2005") +
  theme_minimal()
```

```{r plot 3}
foreclosures %>%
  pivot_longer(
    cols = c(`2002`:`2019`),
    names_to = "year"
  ) %>%
  group_by(year) %>%
  summarize(foreclosures = sum(value, na.rm = T)) %>%
  ggplot(aes(x = year, y = foreclosures)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Year", y = "Homes Sold",
       title = "Foreclosures peaked in 2015") +
  theme_minimal()
```

```{r plot 4}
output[[2]] +
  labs(title = "Sale prices (solid) has increased YoY while\nassessed values (dashed) declined and flattened")
```

## Predicting Overassessments

First we must define an over-assessment. According to [this report](https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf) the legal limit is 50%. We'll use this as the metric. Unfortunately I just don't have time to go in-depth on this homework or I would spend more time on pre-modeling EDA to better determine which characteristics were most important to include. For now I"m going with the "throw spaghetti at the wall" approach and adding as many as I think are initially relevant. 

```{r rf 0}
model_data <- assessments %>%
  left_join(
    sales %>%
      mutate(sale_year = as_date(sale_year)),
    by = c("PARCELNO" = "parcel_num")
)

model_data <- model_data %>%
  filter(year == "2016-01-01", year(sale_date) == 2016, property_c == "401",
         sale_price > 2500, ASSESSEDVALUE > 1250) %>%
  mutate(over = ifelse(ASSESSEDVALUE * 2 > sale_price, "Over", "Not") %>% 
           as.factor())

model_data <- model_data %>%
  left_join(
    parcels %>%
      select(parcel_number, ward, zip_code, use_code_desc, total_square_footage, is_improved, style, year_built, zoning),
    by = c("PARCELNO" = "parcel_number")
  ) %>%
  mutate(is_improved = ifelse(is_improved == 1, T, F)) %>%
  filter(complete.cases(.)) # Removes ~700 rows
```

For this classification problem I will be using a random forest model. The recipe is going to convert NA values to unknown for zip code, style, and zoning values. Other than that I'm not going to mess with too much pre-processing on my first go.

```{r rf 1}
split <- initial_split(model_data)
train <- training(split)
test  <- testing(split)

rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_recipe <- recipe(over ~ sale_price + zip_code + is_improved + style + zoning,
                    data = train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors())

rf_workflow <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_recipe)

rf_fit <- fit(rf_workflow, train)

rf_preds <- augment(rf_fit, train)
```

Let's create a classification matrix for this output using the `performanceEstimation` package that I found. fpr stands for false positive, fnr false negative, ppv predictive positive value, etc. 

```{r rf 2}
performanceEstimation::classificationMetrics(rf_preds$over, rf_preds$.pred_class, metrics = c("fpr", "fnr", "tpr", "tnr", "ppv")) 
```

Not bad!

```{r rf 3}
rf_test <- augment(rf_fit, test)

rf_test %>%
  select(PARCELNO, over, .pred_class) %>%
  count(over, .pred_class) %>%
  mutate(pct = n / sum(n))

performanceEstimation::classificationMetrics(rf_test$over, rf_test$.pred_class, metrics = c("fpr", "fnr", "tpr", "tnr", "ppv")) 
```

Still not bad, that's pretty cool!

```{r rf 4}
roc <- yardstick::roc_curve(data = rf_test, truth = over, .pred_Not)

plot(roc)
```

## Predicting Assessed Values

This model needs more data than just 2019 to avoid overfitting. I'll add in the previous 4 years. Otherwise this process is exactly the same. 

```{r lm 0}
model_data <- assessments %>%
  left_join(
    sales %>%
      mutate(sale_year = as_date(sale_year)),
    by = c("PARCELNO" = "parcel_num")
)

model_data <- model_data %>%
  filter(year(year) <= 2019, year(year) >= 2016, 
         year(sale_date) %in% c(2016:2019), property_c == "401",
         sale_price > 2500, ASSESSEDVALUE > 1250) %>%
  mutate(over = ifelse(ASSESSEDVALUE * 2 > sale_price, "Over", "Not") %>% 
           as.factor())

model_data <- model_data %>%
  left_join(
    parcels %>%
      select(parcel_number, ward, zip_code, use_code_desc, total_square_footage, is_improved, style, year_built, zoning),
    by = c("PARCELNO" = "parcel_number")
  ) %>%
  mutate(is_improved = ifelse(is_improved == 1, T, F)) %>%
  filter(complete.cases(.)) # Removes ~700 rows
```

Same M.O. as the previous model. I'm going to make minimal changes to the data via recipe.

```{r lm 1}
split <- initial_split(model_data)
train <- training(split)
test  <- testing(split)

lm_mod <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

lm_recipe <- recipe(ASSESSEDVALUE ~ sale_price + total_square_footage + 
                      year_built + is_improved + style,
                    data = train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors())

lm_workflow <- workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(lm_recipe)

lm_fit <- fit(lm_workflow, train)

lm_preds <- augment(lm_fit, train %>% filter(year == "2019-01-01"))
```

```{r lm 2}
bind_rows(
  mape(lm_preds, truth = sale_price, estimate = .pred),
  rmse(lm_preds, truth = sale_price, estimate = .pred)
)
```

An average of 72% off across all predictions, and an RMSE of $95,000. How does this predictive power compare to data it hasn't seen?

```{r lm 3}
lm_preds <- augment(lm_fit, test %>% filter(year == "2019-01-01"))

bind_rows(
  mape(lm_preds, truth = sale_price, estimate = .pred),
  rmse(lm_preds, truth = sale_price, estimate = .pred)
)
```

A little better, but it is a smaller sample. In the future I could try an approach that weights the data by recency, since we know the average trend in the housing market is "line go up".
